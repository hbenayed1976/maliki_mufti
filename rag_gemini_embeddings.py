import os
import streamlit as st
import time
import torch
from dotenv import load_dotenv
import asyncio
import sys
import re

# Configuration asyncio pour Windows
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# BibliothÃ¨ques pour le traitement de l'arabe
import arabic_reshaper
from bidi.algorithm import get_display

# LangChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import Document

# --------------------------------------------------
# CONFIGURATION INITIALE
# --------------------------------------------------
load_dotenv()
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# Configuration PyTorch pour Ã©viter les erreurs meta tensor
torch.set_default_dtype(torch.float32)
if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = "cpu"  # Force l'utilisation du CPU pour Ã©viter les problÃ¨mes MPS
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

st.set_page_config(
    page_title="Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„ÙØªØ§ÙˆÙ‰ (PDF + TXT + Coran)",
    page_icon="ğŸ•Œ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --------------------------------------------------
# CONFIGURATION DES EMBEDDINGS CORRIGÃ‰E
# --------------------------------------------------
EMBEDDING_MODELS = {
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": {
        "name": "Sentence Paraphrase Multilangue",
        "description": "ModÃ¨le multilingue optimisÃ© pour la similaritÃ©",
        "model_kwargs": {"device": "cpu"},
        "encode_kwargs": {"normalize_embeddings": True, "batch_size": 8}
    },
    "aubmindlab/bert-base-arabertv2": {
        "name": "AraBERTv2",
        "description": "ModÃ¨le BERT spÃ©cialisÃ© pour l'arabe",
        "model_kwargs": {"device": "cpu"},
        "encode_kwargs": {"normalize_embeddings": True, "batch_size": 4}
    },
    "UBC-NLP/MARBERT": {
        "name": "MARBERT",
        "description": "ModÃ¨le BERT arabe-anglais",
        "model_kwargs": {"device": "cpu"},
        "encode_kwargs": {"normalize_embeddings": True, "batch_size": 4}
    }
}

# --------------------------------------------------
# FONCTION POUR INITIALISER LES EMBEDDINGS DE FAÃ‡ON SÃ‰CURISÃ‰E
# --------------------------------------------------
@st.cache_resource
def load_embedding_model(model_name):
    """Charge le modÃ¨le d'embedding de faÃ§on sÃ©curisÃ©e"""
    try:
        model_config = EMBEDDING_MODELS[model_name]
        
        # Utiliser la configuration exacte du modÃ¨le
        model_kwargs = model_config["model_kwargs"].copy()
        encode_kwargs = model_config["encode_kwargs"].copy()
        
        # CrÃ©er le modÃ¨le d'embedding avec gestion d'erreur
        with st.spinner(f"Chargement du modÃ¨le d'embedding {model_config['name']}..."):
            embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs
            )
            
            # Test du modÃ¨le avec un texte simple
            test_text = "test"
            _ = embeddings.embed_query(test_text)
            
        st.success(f"âœ… ModÃ¨le d'embedding {model_config['name']} chargÃ© avec succÃ¨s")
        return embeddings
        
    except Exception as e:
        st.error(f"âŒ Erreur lors du chargement du modÃ¨le d'embedding: {str(e)}")
        
        # Fallback vers un modÃ¨le plus simple avec configuration minimale
        st.warning("âš ï¸ Tentative de chargement d'un modÃ¨le de secours...")
        try:
            fallback_embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True}
            )
            # Test du modÃ¨le de secours
            _ = fallback_embeddings.embed_query("test")
            st.success("âœ… ModÃ¨le de secours chargÃ© avec succÃ¨s")
            return fallback_embeddings
        except Exception as fallback_error:
            st.error(f"âŒ Impossible de charger mÃªme le modÃ¨le de secours: {str(fallback_error)}")
            
            # Dernier fallback - modÃ¨le trÃ¨s simple
            try:
                simple_embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2"
                )
                _ = simple_embeddings.embed_query("test")
                st.success("âœ… ModÃ¨le trÃ¨s simple chargÃ© avec succÃ¨s")
                return simple_embeddings
            except Exception as final_error:
                st.error(f"âŒ Ã‰chec total: {str(final_error)}")
                return None

# --------------------------------------------------
# SUPPORT DE L'ARABE
# --------------------------------------------------
def setup_arabic_support():
    config = {
        'language': 'Arabic',
        'support_ligatures': True,
        'delete_harakat': False,
        'delete_tatweel': False,
        'shift_harakat_position': False
    }
    return arabic_reshaper.ArabicReshaper(configuration=config)

arabic_reshaped = setup_arabic_support()

def format_arabic(text):
    try:
        if not text or not isinstance(text, str):
            return ""
        return arabic_reshaped.reshape(text.strip())
    except:
        return text

# --------------------------------------------------
# EXTRACTION DES MÃ‰TADONNÃ‰ES DU CORAN
# --------------------------------------------------
def extract_pdf_metadata(doc):
    """Extraire les mÃ©tadonnÃ©es des versets coraniques avec dÃ©tection amÃ©liorÃ©e"""
    metadata = {}
    content = doc.page_content
    
    # Patterns pour identifier les versets et sourates (plus larges)
    surah_pattern = r'Ø³ÙˆØ±Ø©\s+([^\n]+)'
    verse_pattern = r'ï´¿([^ï´¾]+)ï´¾'
    ayah_number_pattern = r'ï´¾\s*\((\d+)\)'
    
    # Extraire le nom de la sourate
    surah_match = re.search(surah_pattern, content)
    if surah_match:
        metadata['sourate'] = surah_match.group(1).strip()
    
    # Extraire les numÃ©ros de versets
    ayah_matches = re.findall(ayah_number_pattern, content)
    if ayah_matches:
        metadata['versets'] = [int(num) for num in ayah_matches]
    
    # DÃ©tection amÃ©liorÃ©e du contenu coranique
    quran_indicators = [
        'ï´¿', 'ï´¾',           # Marqueurs de dÃ©but/fin de verset
        'Ø³ÙˆØ±Ø©',             # Mot "sourate"
        'Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡',          # Basmalah
        'Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡',         # Hamdallah
        'Ù‚Ù„ Ù‡Ùˆ Ø§Ù„Ù„Ù‡ Ø£Ø­Ø¯',    # DÃ©but sourate Al-Ikhlas
        'ØªØ¨Ø§Ø±Ùƒ Ø§Ù„Ø°ÙŠ',       # Mots coraniques courants
        'ÙŠØ§ Ø£ÙŠÙ‡Ø§ Ø§Ù„Ø°ÙŠÙ† Ø¢Ù…Ù†ÙˆØ§', # Appel aux croyants
        'ÙˆØ§Ù„Ù„Ù‡ Ø£Ø¹Ù„Ù…',       # Formule coranique
        'Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†'       # Seigneur des mondes
    ]
    
    # Marquer comme contenu coranique si au moins 2 indicateurs ou si source est tafri3.pdf
    indicators_found = sum(1 for pattern in quran_indicators if pattern in content)
    source_file = doc.metadata.get('source', '')
    
    if indicators_found >= 2 or 'tafri3.pdf' in source_file.lower():
        metadata['type'] = 'coran'
        metadata['source'] = 'tafri3.pdf'
        print(f"ğŸ” Segment coranique dÃ©tectÃ© - Indicateurs: {indicators_found}, Source: {source_file}")
    else:
        print(f"â“ Segment non-coranique - Indicateurs: {indicators_found}, Source: {source_file}, Contenu dÃ©but: {content[:100]}")
    
    return metadata

# --------------------------------------------------
# PROMPT FIQH AMÃ‰LIORÃ‰
# --------------------------------------------------
FIQH_TEMPLATE = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„ÙÙ‚Ù‡ Ø§Ù„Ù…Ø§Ù„ÙƒÙŠ ÙˆØ§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ© ÙˆØ§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ….
Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆÙÙ‚ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ù…Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¥Ù„Ù‰ Ù…ØµØ¯Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:

1- Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆØµØ±ÙŠØ­Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©ØŒ ÙØ§Ø¹Ø±Ø¶Ù‡Ø§ ÙƒÙ…Ø§ Ù‡ÙŠ Ù…Ø¹ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø¥Ù† ÙˆÙØ¬Ø¯ØŒ ÙˆÙ‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "âœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚".
2- Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø¢ÙŠØ§Øª Ù‚Ø±Ø¢Ù†ÙŠØ© Ø°Ø§Øª ØµÙ„Ø©ØŒ ÙØ§Ø°ÙƒØ±Ù‡Ø§ Ù…Ø¹ Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø© ÙˆØ±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ© Ø¥Ù† Ø£Ù…ÙƒÙ†ØŒ ÙˆÙ‚Ù„: "ğŸ“– Ù…Ù† Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…".
3- Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ ÙØ­Ø§ÙˆÙ„ Ø§Ø³ØªÙ†Ø¨Ø§Ø· Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ø´Ø±Ø¹ÙŠ Ø£Ùˆ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ Ù…Ø§ ÙˆØ±Ø¯ ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙÙ‚Ø·ØŒ ÙˆÙ‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "ğŸ“š Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚".
4- Ø¥Ø°Ø§ Ù„Ù… ØªØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø§Ø³ØªÙ†Ø¨Ø§Ø· Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ØŒ ÙÙ‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØªÙƒ ÙƒÙ†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠØŒ ÙˆÙ‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "âš ï¸ Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„ÙŠØ³Øª Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ù„ Ù…Ù† Ù…Ø¹Ø±ÙØªÙŠ ÙƒÙ†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ".

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

Ø§Ù„Ù†ØµÙˆØµ:
{context}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""

# --------------------------------------------------
# CHUNKING POUR TXT ET PDF
# --------------------------------------------------
def custom_chunking_text(documents):
    """DÃ©coupage du TXT avec ***"""
    chunks = []
    for doc in documents:
        if "***" in doc.page_content:
            parts = re.split(r'\s*\*\*\*\s*', doc.page_content)
        else:
            parts = [doc.page_content]
        for i, part in enumerate(parts):
            if part.strip():
                chunk_doc = Document(
                    page_content=part.strip(),
                    metadata=doc.metadata.copy()
                )
                chunk_doc.metadata["segment_id"] = i
                chunks.append(chunk_doc)
    return chunks

def custom_chunking_pdf(documents):
    """DÃ©coupage du PDF avec 2 lignes vides ou plus"""
    chunks = []
    for doc in documents:
        parts = re.split(r'\n\s*\n\s*\n*', doc.page_content)
        for i, part in enumerate(parts):
            if part.strip():
                chunk_doc = Document(
                    page_content=part.strip(),
                    metadata=doc.metadata.copy()
                )
                chunk_doc.metadata["segment_id"] = i
                chunks.append(chunk_doc)
    return chunks

def custom_chunking_quran(documents):
    """DÃ©coupage spÃ©cialisÃ© pour le Coran avec sÃ©parateurs arabes"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", "Û", "Ø›", ".", "ØŒ", "ï´¾", "ï´¿"]  # Ajout de sÃ©parateurs arabes pertinents
    )
    chunks = splitter.split_documents(documents)
    return chunks

# --------------------------------------------------
# INITIALISATION AMÃ‰LIORÃ‰E
# --------------------------------------------------
@st.cache_resource
def init_system(embedding_model_name, llm_choice, google_api_key=None):
    try:
        docs_all = []

        # 1ï¸âƒ£ Charger le PDF des fatwas
        if os.path.exists("fatwa-tounisia.pdf"):
            pdf_loader = PyPDFLoader("fatwa-tounisia.pdf")
            pdf_docs = pdf_loader.load()
            pdf_chunks = custom_chunking_pdf(pdf_docs)
            # Marquer les documents de fatwa
            for chunk in pdf_chunks:
                chunk.metadata["type"] = "fatwa"
                chunk.metadata["source"] = "fatwa-tounisia.pdf"
            docs_all.extend(pdf_chunks)
            st.success(f"âœ… Chargement de {len(pdf_chunks)} segments de fatwas rÃ©ussi")
        else:
            st.warning("âš ï¸ Fichier fatwa-tounisia.pdf introuvable")

        # 2ï¸âƒ£ Charger le TXT
        if os.path.exists("qa.txt"):
            text_loader = TextLoader("qa.txt", encoding="utf-8")
            text_docs = text_loader.load()
            text_chunks = custom_chunking_text(text_docs)
            # Marquer les documents Q&A
            for chunk in text_chunks:
                chunk.metadata["type"] = "qa"
                chunk.metadata["source"] = "qa.txt"
            docs_all.extend(text_chunks)
            st.success(f"âœ… Chargement de {len(text_chunks)} segments Q&A rÃ©ussi")
        else:
            st.warning("âš ï¸ Fichier qa.txt introuvable")

        # 3ï¸âƒ£ Charger le PDF du Coran (tafri3.pdf)
        try:
            pdf_path = "tafri3.pdf"
            if os.path.exists(pdf_path):
                pdf_loader = PyPDFLoader(pdf_path)
                pdf_docs = pdf_loader.load()
                
                # Enrichir les documents PDF avec des mÃ©tadonnÃ©es
                for doc in pdf_docs:
                    # Extraire les mÃ©tadonnÃ©es potentielles du contenu
                    metadata = extract_pdf_metadata(doc)
                    # Mettre Ã  jour les mÃ©tadonnÃ©es du document
                    doc.metadata.update(metadata)
                
                # Utiliser le dÃ©coupage spÃ©cialisÃ© pour le Coran
                quran_chunks = custom_chunking_quran(pdf_docs)
                docs_all.extend(quran_chunks)
                
                st.success(f"âœ… Chargement de {len(quran_chunks)} segments du Coran rÃ©ussi")
                print(f"Chargement de {len(pdf_docs)} pages du PDF rÃ©ussi")
            else:
                st.warning(f"âš ï¸ Fichier PDF du Coran introuvable: {pdf_path}")
                print(f"Fichier PDF introuvable: {pdf_path}")
        except Exception as e:
            st.error(f"âŒ Erreur lors du chargement du PDF du Coran: {e}")
            print(f"Erreur lors du chargement du PDF: {e}")

        if not docs_all:
            st.error("âŒ Aucun document n'a pu Ãªtre chargÃ©")
            return None, None

        # Afficher les statistiques de chargement avec diagnostic
        fatwa_count = len([d for d in docs_all if d.metadata.get("type") == "fatwa"])
        qa_count = len([d for d in docs_all if d.metadata.get("type") == "qa"])
        quran_count = len([d for d in docs_all if d.metadata.get("type") == "coran"])
        unknown_count = len([d for d in docs_all if d.metadata.get("type") not in ["fatwa", "qa", "coran"]])
        
        # Diagnostic dÃ©taillÃ©
        print(f"ğŸ” DIAGNOSTIC DÃ‰TAILLÃ‰:")
        print(f"- Segments initiaux du Coran: {len(quran_chunks) if 'quran_chunks' in locals() else 'N/A'}")
        print(f"- Segments finaux du Coran: {quran_count}")
        print(f"- Segments sans type: {unknown_count}")
        print(f"- Total calculÃ©: {fatwa_count + qa_count + quran_count + unknown_count}")
        print(f"- Total rÃ©el: {len(docs_all)}")
        
        # Types uniques prÃ©sents
        all_types = set(d.metadata.get("type", "unknown") for d in docs_all)
        print(f"- Types dÃ©tectÃ©s: {all_types}")
        
        st.info(f"""
        ğŸ“Š **Statistiques de chargement:**
        - ğŸ“– Segments de fatwas: {fatwa_count}
        - â“ Segments Q&A: {qa_count}
        - ğŸ“œ Segments du Coran: {quran_count}
        - â“ Segments non classifiÃ©s: {unknown_count}
        - ğŸ”¢ **Total**: {len(docs_all)} segments
        
        ğŸ” **VÃ©rification**: {fatwa_count + qa_count + quran_count + unknown_count} segments comptabilisÃ©s
        """)
        
        # Alert si discordance
        if len(quran_chunks if 'quran_chunks' in locals() else []) != quran_count:
            st.warning(f"âš ï¸ Discordance dÃ©tectÃ©e : {len(quran_chunks if 'quran_chunks' in locals() else [])} segments du Coran crÃ©Ã©s initialement, mais seulement {quran_count} classifiÃ©s correctement.")

        # Charger les embeddings de faÃ§on sÃ©curisÃ©e
        embeddings = load_embedding_model(embedding_model_name)
        if embeddings is None:
            st.error("âŒ Impossible de charger le modÃ¨le d'embedding")
            return None, None

        # CrÃ©er le vectorstore avec gestion d'erreur
        try:
            with st.spinner("CrÃ©ation de la base de donnÃ©es vectorielle..."):
                vectorstore = FAISS.from_documents(docs_all, embeddings)
            st.success("âœ… Base de donnÃ©es vectorielle crÃ©Ã©e avec succÃ¨s")
        except Exception as e:
            st.error(f"âŒ Erreur lors de la crÃ©ation du vectorstore: {str(e)}")
            return None, None

        # Choix du LLM - FixÃ© sur Gemini
        if not google_api_key:
            st.error("âŒ GOOGLE_API_KEY manquant pour utiliser Gemini")
            return None, None
        
        try:
            llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash-exp",
                google_api_key=google_api_key,
                temperature=0.1,
                convert_system_message_to_human=True
            )
        except Exception as e:
            st.warning(f"âš ï¸ Erreur avec gemini-2.0-flash-exp, fallback vers gemini-1.5-pro: {str(e)}")
            try:
                llm = ChatGoogleGenerativeAI(
                    model="gemini-1.5-pro",
                    google_api_key=google_api_key,
                    temperature=0.1,
                    convert_system_message_to_human=True
                )
            except Exception as e2:
                st.error(f"âŒ Impossible de charger Gemini: {str(e2)}")
                return None, None

        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(
                search_type="mmr", 
                search_kwargs={
                    "k": 15,
                    "fetch_k": 30,  # Recherche plus large avant MMR
                    "lambda_mult": 0.7  # Balance diversitÃ©/pertinence
                }
            ),
            combine_docs_chain_kwargs={
                "prompt": PromptTemplate(template=FIQH_TEMPLATE, input_variables=["context", "question"])
            },
            return_source_documents=True
        )

        return qa_chain, f"{llm_choice}"
    except Exception as e:
        st.error(f"Erreur: {str(e)}")
        return None, None

# --------------------------------------------------
# INTERFACE
# --------------------------------------------------
def main():
    with st.sidebar:
        st.markdown("## âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")

        embedding_options = [f"{cfg['name']} - {cfg['description']}" for cfg in EMBEDDING_MODELS.values()]
        embedding_keys = list(EMBEDDING_MODELS.keys())
        selected_embedding_model = embedding_keys[st.selectbox("Ø§Ø®ØªØ± Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†:", range(len(embedding_options)), format_func=lambda i: embedding_options[i], index=0)]

        llm_choice = "gemini-2.0-flash-exp"
        
        google_api_key = st.text_input("ğŸ”‘ Google API Key", type="password", value=os.getenv("GOOGLE_API_KEY", ""))

        if st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø©"):
            st.cache_resource.clear()
            if 'qa_chain' in st.session_state:
                del st.session_state['qa_chain']
            st.session_state['embedding_model'] = selected_embedding_model
            st.session_state['llm_choice'] = llm_choice
            st.session_state['google_api_key'] = google_api_key
            st.rerun()

        # Indicateur du modÃ¨le actif
        if 'llm_choice' in st.session_state and 'embedding_model' in st.session_state:
            llm_used = st.session_state.get('llm_choice', 'N/A')
            embedding_used = st.session_state.get('embedding_model', 'N/A')
            st.markdown(f"""
            <div style='background:#f0f8ff; padding:10px; border-radius:8px; margin-top:15px;'>
            <strong>Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ:</strong><br>
            ğŸ§  LLM: {llm_used}<br>
            ğŸ”¤ Embedding: {embedding_used}
            </div>
            """, unsafe_allow_html=True)

        # Informations sur les sources
        st.markdown("""
        ### ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©
        - ğŸ“– **Ø§Ù„ÙØªØ§ÙˆÙ‰**: fatwa-tounisia.pdf
        - â“ **Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø©**: qa.txt
        - ğŸ“œ **Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…**: tafri3.pdf
        """)

        # âœ… Copyright
        st.markdown("<p style='font-size:0.8rem; color:gray; text-align:center'>Â© 2025 Hassan BEN AYED</p>", unsafe_allow_html=True)

    st.markdown("<h1 style='text-align:center'>ğŸ“š Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„ÙØªØ§ÙˆÙ‰ ÙˆØ§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…</h1>", unsafe_allow_html=True)

    if 'qa_chain' not in st.session_state:
        qa_chain, model_name = init_system(selected_embedding_model, llm_choice, google_api_key)
        st.session_state['qa_chain'] = qa_chain
        st.session_state['model_name'] = model_name
        st.session_state['embedding_model'] = selected_embedding_model
        st.session_state['llm_choice'] = llm_choice

    qa_chain = st.session_state.get('qa_chain')
    if not qa_chain:
        st.stop()

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": format_arabic("Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØªØ§ÙˆÙ‰ ÙˆØ§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… Ø§Ù„Ù…ØªØ§Ø­ÙŠÙ†.")}]
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []

    for msg in st.session_state["messages"]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"], unsafe_allow_html=True)

    if prompt := st.chat_input(format_arabic("â“ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§")):
        st.session_state["messages"].append({"role": "user", "content": format_arabic(prompt)})
        with st.chat_message("user"):
            st.markdown(format_arabic(prompt), unsafe_allow_html=True)

        with st.chat_message("assistant"):
            response_container = st.empty()
            full_response = ""
            try:
                result = qa_chain({"question": prompt, "chat_history": st.session_state["chat_history"]})
                answer = result["answer"]
                
                # Analyser les sources utilisÃ©es
                source_docs = result.get("source_documents", [])
                source_types = set()
                for doc in source_docs:
                    doc_type = doc.metadata.get("type", "unknown")
                    source_types.add(doc_type)
                
                # Ajouter information sur les sources utilisÃ©es
                if source_types:
                    sources_info = []
                    if "fatwa" in source_types:
                        sources_info.append("ğŸ“– Ø§Ù„ÙØªØ§ÙˆÙ‰")
                    if "coran" in source_types:
                        sources_info.append("ğŸ“œ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…")
                    if "qa" in source_types:
                        sources_info.append("â“ Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø©")
                    
                    if sources_info:
                        answer += f"\n\n**Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©**: {' | '.join(sources_info)}"
                
                st.session_state["chat_history"].append((prompt, answer))

                # Ajouter info modÃ¨le dans la rÃ©ponse
                llm_used = st.session_state.get('llm_choice', 'Unknown LLM')
                embedding_used = st.session_state.get('embedding_model', 'Unknown Embedding')
                answer += f"\n\n*({llm_used} + {embedding_used})*"

                for char in answer:
                    full_response += char
                    time.sleep(0.01)
                    response_container.markdown(format_arabic(full_response) + "â–Œ", unsafe_allow_html=True)
                response_container.markdown(format_arabic(full_response), unsafe_allow_html=True)
            except Exception as e:
                error_message = f"âš ï¸ Ø®Ø·Ø£: {str(e)}"
                response_container.markdown(error_message)
                full_response = error_message

            st.session_state["messages"].append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
